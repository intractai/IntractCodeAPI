seed: null


data:
  dataset_name_or_path: "iamtarun/python_code_instructions_18k_alpaca"
  split: "train[:40%]"


model:
  model_dir: ".model/"
  model_name: ${oc.env:MODEL_NAME, deepseek-ai/deepseek-coder-1.3b-instruct}
  model_type: standard
  fp16: False 
  bf16: True
  context_length: 256 # 1536
  device: cuda
  use_flash_attention: True
  save_dir: "saved_models/multi_step/"
  load_from_checkpoint: False


train:
  batch_size: 12 # 32
  generation_batch_size: 6 # 32
  max_revision_steps: 2 # Number of times an agent is allowed to revise the code, 1 = single turn
  execution_timeout: 10 # Time limit for code execution in seconds
  execution_length_limit: 500 # 1000 # Character limit for code execution output
  max_gen_length: 32 # 512 # Maximum length of generated code
  use_critic: False # When false, we always assume the code generated by the model is incorrect
  conda_env: "agent_env"
  mask_prompt_labels: False # When prompt labels are masked, the model is trained to predict only responses
  sample_log_frac: 0.05 # Fraction of samples to log
  
  # When true, the model is trained to predict the ground truth code
  # When false, the model is trained to predict the code generated by the model (if deemed correct by the critic)
  use_ground_truth_responses: True 

  # Args for the HuggingFace trainer
  trainer_kwargs:
    output_dir: "saved_models/multi_step/"
    per_device_train_batch_size: 4
    logging_strategy: steps
    logging_steps: 4
    save_steps: 200
    gradient_accumulation_steps: 2
    max_steps: 3600 # Number of train steps
    optim: adamw_bnb_8bit
    gradient_checkpointing: True

  # LoRA / QLoRA params
  lora:
    use_lora: False
    use_qlora: False

    # LoRA params
    lora_r: 4
    lora_alpha: 4
    lora_dropout: 0.01

    # QLoRA params
    bits: 4
    double_quant: True
    quant_type: nf4
    gradient_checkpointing: True