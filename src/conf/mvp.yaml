defaults:
  - config

model:
  model_name: ${oc.env:MODEL_NAME, deepseek-ai/deepseek-coder-1.3b-base} # 1.3b-instruct} # 5.7bmqa-base} # 6.7b-base}
  model_type: standard 
  bf16: True
  context_length: 2048
  device: cuda
  use_flash_attention: True

train:
  max_gen_length: 512 # Maximum length of generated code
  max_retrieval_threads: 4 # Maximum number of threads to use for retriving documentation data

  # What types of training to use
  use_ntp: True
  use_fim: True

  # What to train on
  train_on_code: True
  train_on_docs: True
  train_on_doc_code: True
  train_on_practice_problems: True # Instruction fine-tuning on entire problems
  train_on_verified_solutions: True # NTP/FIM training on just the solutions

  # Parameters for multi-step SFT with verification
  max_query_length: 1536 # Maximum length of input query for instruction fine-tuning
  max_revision_steps: 3 # Maximum number of revisions for instruction fine-tuning + 1
  use_critic: True # Use dummy critic for testing when False
  generation_batch_size: 48 # Number of proposed solutions to generate in a single batch
  execution_timeout: 10 # Time limit for code execution in seconds
  execution_length_limit: 400 # Character limit for code execution output
  conda_env: "agent_env"
  mask_prompt_labels: False # When prompt labels are masked, the model is trained to predict only responses
  sample_log_frac: 0.05 # Fraction of samples to log

  # Trainer parameters
  trainer:
    per_device_train_batch_size: 3
    logging_steps: 8
    logging_strategy: steps
    num_train_epochs: 3
    optim: adamw_bnb_8bit
    
  # Iterative SFT Trainer parameters for multi-step SFT with verification
  iterative_sft_trainer:
    per_device_train_batch_size: 4
    logging_steps: 4
    gradient_accumulation_steps: 2
    num_train_epochs: 3
    logging_strategy: steps
    optim: adamw_bnb_8bit
    gradient_checkpointing: True