defaults:
  - prompts/describe_library_doc@_here_
  - prompts/generate_library_problems@_here_
  - rag: default


server:
  host: 127.0.0.1
  port: 8000


session:
  max_active_sessions: 4
  eviction_interval: 1800 # Check for idle sessions every 30 minutes
  max_session_idle_time: 600 # Evict sessions that have been idle for 10 minutes
  max_finetune_threads: 2 # Global maximum number of threads for fine-tuning


database:
  path: "../db.sqlite3"


seed: null


model:
  save_model_dir: "../saved_models/"
  model_dir: "../.model/"
  model_name: ${oc.env:MODEL_NAME, deepseek-ai/deepseek-coder-1.3b-base}
  model_type: standard
  fp16: False
  bf16: True
  context_length: 512
  device: cuda
  use_flash_attention: True

  # LOARA Parameters, if selected
  lora_r: 64
  lora_alpha: 16
  lora_dropout: 0.01

  # QLOARA Parameters, if selected
  bits: 4
  double_quant: True
  quant_type: nf4
  optim: paged_adamw_32bit
  gradient_checkpointing: True


inference:
  max_gen_length: 128


train:
  max_gen_length: 128 # Maximum length of generated code

  # Parameters for retrieving documentation and generating problems
  max_retrieval_threads: 4

  # Parameters for multi-step SFT with verification
  max_query_length: 512 # Maximum length of input query for instruction fine-tuning
  max_revision_steps: 2 # Maximum number of revisions for instruction fine-tuning + 1
  use_critic: False # Use dummy critic for testing when False
  generation_batch_size: 6 # Number of proposed solutions to generate in a single batch
  execution_timeout: 10 # Time limit for code execution in seconds
  execution_length_limit: 100 # Character limit for code execution output
  conda_env: "agent_env"
  mask_prompt_labels: True # When prompt labels are masked, the model is trained to predict only responses
  sample_log_frac: 0.05 # Fraction of samples to log

  # What types of training to use
  use_ntp: True
  use_fim: True

  # What to train on
  train_on_code: True
  train_on_docs: False
  train_on_doc_code: False
  train_on_practice_problems: False # Instruction fine-tuning
  train_on_verified_solutions: False
  train_on_documents: False

  # Trainer parameters
  trainer:
    do_train: True
    per_device_train_batch_size: 1
    num_train_epochs: 2
    remove_unused_columns: False
    include_inputs_for_metrics: True
    logging_steps: 8
    logging_strategy: steps
    optim: adamw_bnb_8bit
    
  # Iterative SFT Trainer parameters for multi-step SFT with verification
  iterative_sft_trainer:
    per_device_train_batch_size: 4
    logging_steps: 4
    logging_strategy: steps
    num_train_epochs: 2
    optim: adamw_bnb_8bit
    gradient_checkpointing: True


cache:
  cache_dir: "../.cache/"
  cache_documents: True
  cache_web: True


rag:
  enabled: False
  max_gen_context_length: 192
  chunk_size: 512