defaults:
  - config

wandb: False

model:
  save_model_dir: "../saved_models/"
  model_dir: "../.model/"
  model_name: ${oc.env:MODEL_NAME, deepseek-ai/deepseek-coder-1.3b-instruct} # 1.3b-instruct} # 5.7bmqa-base} # 6.7b-base}
  model_type: standard 
  bf16: True
  context_length: 1536
  device: cuda
  use_flash_attention: True

eval:
  metrics_output_dir: "metrics/"

  # Benchmarks
  human_eval: False

  # Tasks
  custom_tasks: True

  # Eval params
  # total_epochs: 2
  # epochs_per_project: 2
  # n_finetune_projects: 5
  # n_ood_projects: 3


train:
  max_gen_length: 512 # Maximum length of generated code
  max_retrieval_threads: 4 # Maximum number of threads to use for retriving documentation data

  # What types of training to use
  use_ntp: True
  use_fim: True

  # What to train on
  train_on_code: True
  train_on_documents: True
  train_on_docs: False
  train_on_doc_code: False
  train_on_practice_problems: False # Instruction fine-tuning on entire problems
  train_on_verified_solutions: False # NTP/FIM training on just the solutions

  # Parameters for multi-step SFT with verification
  max_query_length: 768 # 1536 # Maximum length of input query for instruction fine-tuning
  max_revision_steps: 3 # Maximum number of revisions for instruction fine-tuning + 1
  use_critic: True # Use dummy critic for testing when False
  generation_batch_size: 16 # 48 # Number of proposed solutions to generate in a single batch
  execution_timeout: 10 # Time limit for code execution in seconds
  execution_length_limit: 400 # Character limit for code execution output
  conda_env: "agent_env"
  mask_prompt_labels: False # When prompt labels are masked, the model is trained to predict only responses
  sample_log_frac: 0.05 # Fraction of samples to log

  # Trainer parameters
  trainer:
    per_device_train_batch_size: 2
    per_device_eval_batch_size: 8
    logging_steps: 8
    logging_strategy: steps
    num_train_epochs: 2
    optim: adamw_bnb_8bit
    
  # Iterative SFT Trainer parameters for multi-step SFT with verification
  iterative_sft_trainer:
    per_device_train_batch_size: 4
    logging_steps: 4
    gradient_accumulation_steps: 2
    num_train_epochs: 1
    logging_strategy: steps
    optim: adamw_bnb_8bit
    gradient_checkpointing: True